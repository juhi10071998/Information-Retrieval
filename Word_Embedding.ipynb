{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word_Embedding.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DONbL8wAHhMD",
        "outputId": "16dc3de3-73a2-4199-9cea-3ac02780d240"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmxNbJM2Hq9u"
      },
      "source": [
        "\n",
        "import json\n",
        "import os\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import multiprocessing\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "from gensim.parsing.preprocessing import stem_text\n",
        "import re\n",
        "import sys\n",
        "## Converts to binary code\n",
        "import pandas as pd\n",
        "import base64\n",
        "import numpy as np\n",
        "# import imageio\n",
        "import os\n",
        "import scipy\n",
        "import gensim\n",
        "import re\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import word_tokenize\n",
        "import gensim.corpora as corpora\n",
        "from gensim.models import KeyedVectors\n",
        "# Load vectors directly from the file\n",
        "from gensim.models import TfidfModel\n",
        "from gensim.corpora import Dictionary\n",
        "import pprint\n",
        "import pickle\n",
        "## Converts to binary code\n",
        "import pandas as pd\n",
        "import base64\n",
        "import numpy as np\n",
        "# import imageio\n",
        "import os\n",
        "import scipy\n",
        "import gensim\n",
        "import re\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import word_tokenize\n",
        "import gensim.corpora as corpora\n",
        "import itertools"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "556gjJRyHrAU"
      },
      "source": [
        "final = np.load('/content/gdrive/My Drive/IR Assignment/uniq_words.npy',allow_pickle=True)\n",
        "model = gensim.models.KeyedVectors.load_word2vec_format('/content/gdrive/My Drive/IR Assignment/GoogleNews-vectors-negative300.bin', binary=True)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPa2ZEt7HrCs"
      },
      "source": [
        "\n",
        "similarity_score={}\n",
        "for word in final:\n",
        "  try:\n",
        "    similar = model.most_similar(positive=[word],topn=5)\n",
        "    temp = [[word,1]]\n",
        "    for x in similar:\n",
        "      word2 = x[0].lower()\n",
        "      if word2 in final and word2!=word:\n",
        "        temp.append([word2,x[1]])\n",
        "    similarity_score[word]=temp\n",
        "  except:\n",
        "    similarity_score[word]=[[word,1]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yrh5FnN2HrFJ"
      },
      "source": [
        "\n",
        "print(similarity_score['sorry'][2][0]) # word\n",
        "print(similarity_score['sorry'][2][1]) # score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_AZd9EXsHrHO"
      },
      "source": [
        "pickle_out = open(\"/content/gdrive/My Drive/IR Assignment/similarity_score.pickle\",\"wb\")\n",
        "pickle.dump(similarity_score, pickle_out)\n",
        "pickle_out.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}