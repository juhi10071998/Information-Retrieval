{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Preprocessing.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Lm593-PGVUm",
        "outputId": "d03d69de-dc5d-49ef-df54-458bf1ccd6b8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsXyt8BhGipn",
        "outputId": "daa288c2-4c68-4e63-b9b7-62c58c5299f8"
      },
      "source": [
        "import pandas as pd\n",
        "import base64\n",
        "import numpy as np\n",
        "# import imageio\n",
        "import os\n",
        "import scipy\n",
        "import gensim\n",
        "import re\n",
        "from copy import deepcopy\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import word_tokenize\n",
        "import gensim.corpora as corpora\n",
        "import itertools\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "stops = stopwords.words('english')\n",
        "\n",
        "''' This method removes all kinds of line breaks. '''\n",
        "def removeLineBreaks(tweet):\n",
        "    return re.sub(\"\\n\\r|\\r\\n|\\n|\\r\",\" \", tweet)\n",
        "\n",
        "''' This method removes all the url's in the tweet'''\n",
        "def removeURLs(tweet):\n",
        "    return re.sub(\"(http|ftp|https)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?\", \" \", tweet)\n",
        "\n",
        "''' This method removes all emojis from the tweet'''\n",
        "def removeEmojis(tweet):\n",
        "    tweet = tweet.encode('ascii', 'ignore').decode('ascii')\n",
        "    return tweet\n",
        "\n",
        "''' This method checks if the tweet is a retweet or not.\n",
        "    a retweet contains RT @***** '''\n",
        "def isRetweet(tweet):\n",
        "    retweet = re.compile(\"RT @[A-Za-z0-9]*:\")\n",
        "    retweet.match(tweet)\n",
        "\n",
        "    return bool(re.search(\"RT @[A-Za-z0-9]*:\", tweet))\n",
        "\n",
        "''' This method removes the retweet tag from tweets'''\n",
        "def removeRTtag(tweet):\n",
        "    return re.sub(\"RT @[A-Za-z0-9]*: \", \" \", tweet)\n",
        "\n",
        "''' This method removes all the mentions.\n",
        "    mentions are usually with @'''\n",
        "def removeMentions(tweet):\n",
        "    return re.sub(\"@[A-Za-z0-9]*\", \" \", tweet)\n",
        "\n",
        "''' This method removes multiple spaces.'''\n",
        "def removeMultipleSpaces(tweet):\n",
        "    return re.sub(\" +\", \" \", tweet)\n",
        "\n",
        "''' This method turns the tweets into lowercase. '''\n",
        "def lowercasetweet(tweet):\n",
        "    return tweet.lower()\n",
        "\n",
        "''' This method removes all the punctuations from the tweet.'''\n",
        "def removePunctuations(tweet):\n",
        "    return re.sub(\"[.,!'\\\";:?…]+\", \" \", tweet)\n",
        "\n",
        "''' This method removes special characters from tweets.'''\n",
        "def removeSpecialCharacters(tweet):\n",
        "    return re.sub(\"[@#$%^*(){}\\\\\\<>\\[\\]~/|=\\+\\-&_¿ߒ]+\",\" \", tweet)\n",
        "\n",
        "''' This method removes alpha-numeric charcters from the tweet.'''\n",
        "def removeAlphaNumeric(tweet):\n",
        "    # return re.sub(\"[A-Za-z]+[0-9]+\", \"\", tweet)\n",
        "    return re.sub(\"[0-9]+\", \"\", tweet)\n",
        "\n",
        "''' Lemmatization using nltk. '''\n",
        "def lemmatizeTweet(tweet):\n",
        "    return [WordNetLemmatizer().lemmatize(token) for token in word_tokenize(tweet)]\n",
        "\n",
        "def cleanData(text, lowercase = False, remove_stops = False, stemming = False, lemmatization = False):\n",
        "    txt = str(text)\n",
        "\n",
        "    # Replace apostrophes with standard lexicons\n",
        "    txt = txt.replace(\"isn't\", \"is not\")\n",
        "    txt = txt.replace(\"aren't\", \"are not\")\n",
        "    txt = txt.replace(\"ain't\", \"am not\")\n",
        "    txt = txt.replace(\"won't\", \"will not\")\n",
        "    txt = txt.replace(\"didn't\", \"did not\")\n",
        "    txt = txt.replace(\"shan't\", \"shall not\")\n",
        "    txt = txt.replace(\"haven't\", \"have not\")\n",
        "    txt = txt.replace(\"hadn't\", \"had not\")\n",
        "    txt = txt.replace(\"hasn't\", \"has not\")\n",
        "    txt = txt.replace(\"don't\", \"do not\")\n",
        "    txt = txt.replace(\"wasn't\", \"was not\")\n",
        "    txt = txt.replace(\"weren't\", \"were not\")\n",
        "    txt = txt.replace(\"doesn't\", \"does not\")\n",
        "    txt = txt.replace(\"'s\", \" is\")\n",
        "    txt = txt.replace(\"'re\", \" are\")\n",
        "    txt = txt.replace(\"'m\", \" am\")\n",
        "    txt = txt.replace(\"'d\", \" would\")\n",
        "    txt = txt.replace(\"'ll\", \" will\")\n",
        "\n",
        "    # Emoji replacement\n",
        "    txt = re.sub(r':\\)',r' happy ',txt)\n",
        "    txt = re.sub(r':D',r' happy ',txt)\n",
        "    txt = re.sub(r':P',r' happy ',txt)\n",
        "    txt = re.sub(r':\\(',r' sad ',txt)\n",
        "\n",
        "    # Replace words like sooooooo with so\n",
        "    txt = ''.join(''.join(s)[:2] for _, s in itertools.groupby(txt))\n",
        "    return txt"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DGUnttEGuyk"
      },
      "source": [
        "final = np.load('/content/gdrive/My Drive/IR Assignment/IR_assignment.npy',allow_pickle=True)\n",
        "uniq_words=[]\n",
        "count={}\n",
        "import os\n",
        "import pickle\n",
        "cd={}\n",
        "index={}\n",
        "for line in final:\n",
        "    count_document={}\n",
        "    cnt=0\n",
        "    ll = lemmatizeTweet(removeMultipleSpaces(removeURLs(removeMentions(removeEmojis(removeSpecialCharacters(removePunctuations(removeAlphaNumeric(cleanData(removeLineBreaks(line[1].lower()))))))))))\n",
        "    for word in ll:\n",
        "        if word not in stops:\n",
        "          cnt+=1\n",
        "          try:\n",
        "            count_document[word]+=1\n",
        "          except:\n",
        "            count_document[word]=1\n",
        "          if word not in uniq_words:\n",
        "            uniq_words.append(word)\n",
        "            count[word]=1\n",
        "            index[word]=[]\n",
        "          else:\n",
        "            count[word]+=1\n",
        "          if str(line[0]+':'+str(cnt)) not in index[word]:\n",
        "             index[word].append(str(line[0]+':'+str(cnt)))\n",
        "    cd[line[0]]=deepcopy(count_document)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KgU6iw-GvTh",
        "outputId": "ef0a5c47-ec12-4306-a95c-d03554fccd87"
      },
      "source": [
        "print(count['happy'])\n",
        "print(index['party'])\n",
        "print(cd['1']['sorry'])\n",
        "print(len(uniq_words))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "838\n",
            "['1:14', '27:12', '126:161', '225:121', '344:39', '381:9', '436:86', '475:106', '475:108', '475:130', '475:143', '513:187', '513:191', '516:62', '561:33', '737:71', '752:13', '837:23', '986:38', '1164:20', '1200:9', '1497:23', '1863:73', '1863:434', '2202:128', '2317:7', '2317:14', '2492:22', '2673:5', '2773:57', '2920:77', '2990:8', '3026:70', '3039:57', '3203:7', '3223:305', '3909:15', '3909:54', '3942:30', '3960:46', '3996:30', '4065:71', '4065:160', '4065:183', '4278:30', '4278:36', '4506:25', '4527:16', '5020:48', '5159:13', '5210:146', '5268:78', '5371:199', '5638:566', '5638:793', '5638:797', '6158:93', '6158:142', '6169:36']\n",
            "1\n",
            "14337\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7PnM70xlGvWR",
        "outputId": "8d8166e6-d3bf-4b2f-cd1f-78b333ccbf64"
      },
      "source": [
        "print(final[0])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['1'\n",
            " 'I\\'m sorry to hear about the state you\\'re in. Thank you for opening up.\\n\\nA few personal thoughts:\\n\\nre: college - if you have this perception of college, you\\'ll do well in life :) While the party atmosphere is the most \"visible\" part of any campus, I can guarantee that there\\'s people around you who feel the same way about it as you do. Try to connect with them somehow. You\\'re not alone, I guarantee it. Life gets better once you\\'re done college, however enjoy it while you\\'re there. If your courseload is too heavy, try to even it out, and consider taking summer courses - you might not finish as fast as you\\'d like, however you\\'ll still finish faster than normal. Take electives you\\'re interested in and enjoy.\\n\\nre: relationships - I know it isn\\'t easy, however I usually try to tell people that a good number of people that I know that are in relationships wish they were single, and a good number of single people that I know wish they were in relationships - no matter what your situation, I know somebody who\\'s jealous of it :) Love and loving isn\\'t easy, but it is something worth pursuing. If you went through a bad situation with your parents, remember that you can learn from their mistakes. \\n\\nThings will get better over time. The most important thing is talking about it, and if you\\'re feeling suicidal, don\\'t be afraid to contact some of the helplines listed in the subreddit. The people on those helplines are there to help and listen and want to hear about what you\\'re going through, and you aren\\'t wasting their time by calling. ']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uz8u8eGIGvYo"
      },
      "source": [
        "\n",
        "pickle_out = open(\"/content/gdrive/My Drive/IR Assignment/inverted_index.pickle\",\"wb\")\n",
        "pickle.dump(index, pickle_out)\n",
        "pickle_out.close()\n",
        "pickle_out = open(\"/content/gdrive/My Drive/IR Assignment/count_word.pickle\",\"wb\")\n",
        "pickle.dump(count, pickle_out)\n",
        "pickle_out.close()\n",
        "np.save(\"/content/gdrive/My Drive/IR Assignment/uniq_words.npy\",uniq_words)\n",
        "pickle_out = open(\"/content/gdrive/My Drive/IR Assignment/count_per_document.pickle\",\"wb\")\n",
        "pickle.dump(cd, pickle_out)\n",
        "pickle_out.close()"
      ],
      "execution_count": 7,
      "outputs": []
    }
  ]
}